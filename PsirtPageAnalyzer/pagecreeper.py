import requests
import bs4
import html2text
import re
import logging
from requests.exceptions import *
from multiprocessing import Pool
from PsirtPageAnalyzer.models import *
from django.db import IntegrityError


class PsirtPageCreeper():
    lenovoSupportHome = 'http://support.lenovo.com'
    severityFlag = 'Severity:'
    cveRep = r'CVE-\d{4}-\d{4}'

    vulCollection = {}

    def normalize_str(self, inputStr):
        if inputStr is None:
            return ''
        else:
            # print(inputStr.encode('utf-8'))
            str = inputStr.replace('\n', '')\
                          .replace('\\xa0', '')\
                          .replace('\u2212', '')\
                          .replace('\u201c', '\"')\
                          .replace('\u201d', '\"')\
                          .replace(':', '')\
                          .strip()
            return ' '.join(str.split())


    def extract_cve_code(self, content_text):
        return re.findall(self.cveRep, content_text, re.M | re.I)

    def convert_severity_to_integer(self, severity):
        if severity == 'High':
            return 1
        elif severity == 'Medium':
            return 2
        else:
            return 3

    def convert_status_to_integer(self, status):
        if status.replace(' ', '') == 'Notaffected':
            return 0
        elif status == 'Affected':
            return 1
        else:
            return 2

    def parse_tbl_row(self, table_row):
        td_elems = table_row.find_all('td')
        if len(td_elems) == 0: return None

        strs = list(td_elems[0].strings)
        lenovo_code = self.normalize_str(strs[0])
        description = self.normalize_str(strs[1])

        a_elems = td_elems[0].find_all('a')
        link = self.lenovoSupportHome + a_elems[0]['href']

        firstDate = self.normalize_str(td_elems[1].string)
        lastDate = self.normalize_str(td_elems[2].string)

        return Vulnerability(lenovoCode=lenovo_code, description=description,
                             link=link, firstPublishedDate=firstDate,
                             lastUpdatedDate=lastDate)

    def parse_vuls_table(self, vul_table):
        vuls = []
        items = vul_table.find_all('tr')

        # skip the table header line
        for index in range(1, len(items)):
            vul = self.parse_tbl_row(items[index])
            if vul is not None:
                vuls.append(vul)
        return vuls

    def parse_vul_detail(self, vul, entire_vul_html):
        soup = bs4.BeautifulSoup(entire_vul_html, 'html.parser')
        # complete the vulnerability information then save to DB
        pure_text_of_content = html2text.html2text(soup.get_text())  # convert to pure text
        vul.severity = self.extract_severity(pure_text_of_content)
        vul.cveCodes = repr(self.extract_cve_code(pure_text_of_content))
        try:
            vul.save()
        except IntegrityError:
            print('duplicate vulnerability code occurred:' + vul.lenovoCode)

        # find all relation business unit and save the business unit information
        bu_map = self.deal_with_bu_info(vul, soup)

        # deal with product, business unit and vulnerability relationship
        prods_content_block_list = soup.find_all('div', id='NewTileListContent')
        for buKey in bu_map.keys():
            self.parse_products_detail(buKey, bu_map[buKey], vul, prods_content_block_list)

    def extract_severity(self, pure_text_of_content):
        start_pos = pure_text_of_content.find(self.severityFlag)
        end_pos = pure_text_of_content.find(' ', start_pos + len(self.severityFlag) + 1)
        severity = pure_text_of_content[start_pos + len(self.severityFlag) + 1: end_pos]
        return self.convert_severity_to_integer(severity)

    def deal_with_bu_info(self, vul, content):
        bu_map = {}
        bu_block_elem = content.find_all(id='NewTileListComponent')
        if bu_block_elem is not None and len(bu_block_elem) > 0:
            for ulElem in bu_block_elem[0].find_all('ul'):
                for liElem in ulElem.find_all('li'):
                    bu, created = BusinessUnit.objects.get_or_create(name=self.normalize_str(liElem.get_text()))
                    bu.save()
                    bu_map[self.normalize_str(liElem['itemindex'])] = bu

        return bu_map

    def parse_products_detail(self, buKey, bu, vul, productsContentBlockList):
        bu_prod_block_list = []
        for productsContentBlock in productsContentBlockList:
            bu_prod_block_list = productsContentBlock.find_all('div', itemindex=buKey)
            if len(bu_prod_block_list) != 0:
                break

        if len(bu_prod_block_list) != 0:
            products_table = bu_prod_block_list[0].table
            products_rows = products_table.find_all('tr')
            if len(products_rows) > 0:
                for index in range(1, len(products_rows)):
                    row = products_rows[index]
                    columns = row.find_all('td')
                    product, created = Product.objects.get_or_create(
                        name=self.normalize_str(columns[0].get_text()))
                    if len(columns) == 2:
                        product.status = self.convert_status_to_integer(columns[1].get_text())
                    elif len(columns) == 4:
                        product.status = self.convert_status_to_integer(columns[1].get_text())
                        product.fixedVersion = self.normalize_str(columns[2].get_text())
                        product.downloadLink = self.normalize_str(columns[3].get_text())
                    else:
                        pass

                    product.save()

                    bp, created = BUAndProdRelationship.objects.get_or_create(bu=bu, product=product)
                    if created : bp.save()
                    pv, created = ProdAndVulRelationship.objects.get_or_create(product=product, vul=vul)
                    if created: pv.save()

    def load_page(self, url):
        print("loading " + url)

        # try three times to avoid the timeout case
        for i in range(3):
            try:
                response = requests.get(url, timeout=30)
                if response.status_code == 200:
                    soup = bs4.BeautifulSoup(response.text)
                    content = soup.select('div.content-wrapper')[0]
                    return content
            except (ReadTimeout, Timeout, ConnectTimeout):
                print("get a timeout exception on loading: " + url)
                continue

        return ''

    def process_vul_detail_page(self, vul):
        try:
            entire_content = self.load_page(vul.link)
            return entire_content.prettify(formatter=None)
        except Exception:
            logging.exception("arg is %s" % vul.lenovoCode)

    def creep(self):
        try:
            home_page_html = self.load_page(self.lenovoSupportHome + '/us/en/product_security')
            vuls = self.parse_vuls_table(home_page_html.table)

            # go though all vulnerability element
            pool = Pool(16)
            vuls_content = pool.map(self.process_vul_detail_page, vuls)
            pool.close()
            pool.join()

            for index in range(len(vuls)):
                self.parse_vul_detail(vuls[index], vuls_content[index])
                print("completed the " + vuls[index].lenovoCode)
            return True
        except Exception as x:
            logging.exception("internal error: %s" % x.atleast)
            return False
