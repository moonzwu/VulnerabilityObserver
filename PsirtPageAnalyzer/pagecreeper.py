import requests
import bs4
import html2text
import re
import logging
import traceback
import copy
from requests.exceptions import *
from multiprocessing import Pool
from PsirtPageAnalyzer.models import *
from django.db import IntegrityError
from PsirtPageAnalyzer.utils import DataConvert, StringUtil


class PsirtPageCreeper():
    lenovoSupportHome = 'http://support.lenovo.com'
    severityRep = 'Severity[" "\\n]{0,1}:[" "]{0,1}\w*'
    cveRep = r"CVE-\d{4}-\d{4}"

    def extract_cve_code(self, content_text):
        return re.findall(self.cveRep, content_text, re.M | re.I)

    def selectValidCVEContent(self, soup):
        lis = soup.find_all('li')
        for li in lis:
            validCVEContent = html2text.html2text(li.get_text())
            if (validCVEContent.find("CVE ID:") != -1):
                return validCVEContent

        # deal with the no <li> tag for CVE ID case
        vulTxtContent = html2text.html2text(soup.get_text())
        startPos = vulTxtContent.find("CVE ID:")
        if (startPos != -1):
            return vulTxtContent[startPos:]
        else:
            return ''

    def parse_tbl_row(self, table_row):
        td_elems = table_row.find_all('td')
        if len(td_elems) == 0: return None

        strs = list(td_elems[0].strings)
        lenovo_code = StringUtil.normalize(strs[0])
        description = StringUtil.normalize(strs[1])

        a_elems = td_elems[0].find_all('a')
        link = self.lenovoSupportHome + a_elems[0]['href']

        firstDate = StringUtil.normalize(td_elems[1].string)
        lastDate = StringUtil.normalize(td_elems[2].string)

        return Vulnerability(lenovoCode=lenovo_code, description=description,
                             link=link, firstPublishedDate=firstDate,
                             lastUpdatedDate=lastDate)

    def parse_vuls_table(self, vul_table):
        vuls = []
        items = vul_table.find_all('tr')

        # skip the table header line
        for index in range(1, len(items)):
            vul = self.parse_tbl_row(items[index])
            if vul is not None:
                vuls.append(vul)
        return vuls

    def parse_vul_detail(self, vul, entire_vul_html):
        soup = bs4.BeautifulSoup(entire_vul_html, 'html.parser')
        # complete the vulnerability information then save to DB
        pure_text_of_content = html2text.html2text(soup.get_text())  # convert to pure text
        vul.severity = self.extract_severity(pure_text_of_content)
        vul.cveCodes = repr(self.extract_cve_code(content_text=self.selectValidCVEContent(soup=soup)))
        existed_vul = None
        try:
            existed_vul, created = Vulnerability.objects.get_or_create(lenovoCode=vul.lenovoCode)
            existed_vul.lenovoCode = vul.lenovoCode
            existed_vul.description = vul.description
            existed_vul.link = vul.link
            existed_vul.firstPublishedDate = vul.firstPublishedDate
            existed_vul.lastUpdatedDate = vul.lastUpdatedDate
            existed_vul.severity = vul.severity
            existed_vul.cveCodes = vul.cveCodes

            existed_vul.save()
        except IntegrityError:
            print('duplicate vulnerability code occurred:' + vul.lenovoCode)
            print('%s' % repr(vul))

        # find all relation business unit and save the business unit information
        bu_map = self.deal_with_bu_info(existed_vul, soup)

        # deal with product, business unit and vulnerability relationship
        prods_content_block_list = soup.find_all('div', id='NewTileListContent')
        for buKey in bu_map.keys():
            self.parse_products_detail(buKey, bu_map[buKey], existed_vul, prods_content_block_list)

    def extract_severity(self, pure_text_of_content):
        result = re.findall(self.severityRep, pure_text_of_content)
        if len(result) == 0:
            return ""
        else:
            severity = result[0].split(" ")[-1]
            return DataConvert.convert_severity_to_integer(severity)

    def deal_with_bu_info(self, vul, content):
        bu_map = {}
        bu_block_elem = content.find_all(id='NewTileListComponent')
        if bu_block_elem is not None and len(bu_block_elem) > 0:
            for ulElem in bu_block_elem[0].find_all('ul'):
                for liElem in ulElem.find_all('li'):
                    bu, created = BusinessUnit.objects.get_or_create(name=StringUtil.normalize(liElem.get_text()))
                    bu.save()
                    bu_map[StringUtil.normalize(liElem['itemindex'])] = bu

        return bu_map

    def parse_products_detail(self, buKey, bu, vul, productsContentBlockList):
        bu_prod_block_list = []
        for productsContentBlock in productsContentBlockList:
            bu_prod_block_list = productsContentBlock.find_all('div', itemindex=buKey)
            if len(bu_prod_block_list) != 0:
                break

        if len(bu_prod_block_list) != 0:
            products_table = bu_prod_block_list[0].table
            products_rows = products_table.find_all('tr')
            if len(products_rows) > 0:
                for index in range(1, len(products_rows)):
                    row = products_rows[index]
                    columns = row.find_all('td')
                    product, created = Product.objects.get_or_create(
                        name=StringUtil.normalize(columns[0].get_text()))
                    product.save()

                    bp, created = BUAndProdRelationship.objects.get_or_create(bu=bu, product=product)
                    if created: bp.save()

                    pv, created = ProdAndVulRelationship.objects.get_or_create(product=product, vul=vul)
                    if created:
                        if len(columns) == 2:
                            pv.status = DataConvert.convert_status_to_integer(columns[1].get_text())
                        elif len(columns) == 4:
                            pv.status = DataConvert.convert_status_to_integer(columns[1].get_text())
                            pv.fixedVersion = StringUtil.normalize(columns[2].get_text())
                            pv.downloadLink = StringUtil.normalize(columns[3].get_text(), ep=False)
                        else:
                            pass
                        pv.save()

    def load_page(self, url):
        print("loading " + url)

        # try three times to avoid the timeout case
        for i in range(3):
            try:
                response = requests.get(url, timeout=30)
                if response.status_code == 200:
                    soup = bs4.BeautifulSoup(response.text)
                    content = soup.select('div.content-wrapper')[0]
                    return content
            except (ReadTimeout, Timeout, ConnectTimeout):
                print("get a timeout exception on loading: " + url)
                continue

        return ''

    def process_vul_detail_page(self, vul):
        try:
            entire_content = self.load_page(vul.link)
            return entire_content.prettify(formatter=None)
        except Exception:
            logging.exception("arg is %s" % vul.lenovoCode)

    def creep(self):
        try:
            home_page_html = self.load_page(self.lenovoSupportHome + '/us/en/product_security')
            vuls = self.parse_vuls_table(home_page_html.table)

            # go though all vulnerability element
            pool = Pool(16)
            vuls_content = pool.map(self.process_vul_detail_page, vuls)
            pool.close()
            pool.join()

            for index in range(len(vuls)):
                self.parse_vul_detail(vuls[index], vuls_content[index])
                print("completed the " + vuls[index].lenovoCode)
            return True
        except Exception:
            traceback.print_exc()
            return False
